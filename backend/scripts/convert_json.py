import google.generativeai as genai
import json
import os
import re
import time
from datetime import datetime
from dotenv import load_dotenv
import sqlite3

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±) - backend í´ë” ê¸°ì¤€
load_dotenv("../.env") 

# --- 0. ëª¨ë¸ ì„¤ì • ë° API í‚¤ ë¡œë“œ ---
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    print("ì˜¤ë¥˜: GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜, ì½”ë“œì— ì§ì ‘ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    exit()
genai.configure(api_key=API_KEY)

# í”„ë¡œì„¸ìŠ¤ ID ê°€ì ¸ì˜¤ê¸° (í™˜ê²½ë³€ìˆ˜ì—ì„œ)
PROCESS_ID = int(os.getenv('PROCESS_ID', '0'))

# ë¡œê·¸ ì„¤ì • (í”„ë¡œì„¸ìŠ¤ë³„)
import logging
from datetime import datetime

def setup_logging():
    """í”„ë¡œì„¸ìŠ¤ë³„ ë¡œê¹… ì„¤ì •"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # í”„ë¡œì„¸ìŠ¤ë³„ ë¡œê±° ìƒì„±
    logger_name = f'convert_json_process_{PROCESS_ID}'
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (í”„ë¡œì„¸ìŠ¤ë³„)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_handler = logging.FileHandler(
        f"{log_dir}/convert_json_p{PROCESS_ID}_{timestamp}.log", 
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° (í”„ë¡œì„¸ìŠ¤ ID í¬í•¨)
    formatter = logging.Formatter(
        f'%(asctime)s - P{PROCESS_ID} - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ë¡œê±° ì´ˆê¸°í™”
logger = setup_logging()

logger.info(f"âœ… Gemini API í‚¤ ì—°ê²° ì™„ë£Œ (í”„ë¡œì„¸ìŠ¤ {PROCESS_ID}ë²ˆ)")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ ì½ê¸°
PROMPT_FILE_PATH = '../prompts/prompt_report.txt'

try:
    with open(PROMPT_FILE_PATH, 'r', encoding='utf-8') as f:
        prompt_template = f.read()
    print(f"'{PROMPT_FILE_PATH}' íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì–´ì™”ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: '{PROMPT_FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    prompt_template = """
    ## íƒêµ¬ë³´ê³ ì„œ í…ìŠ¤íŠ¸ êµ¬ì¡°í™” ìš”ì²­

    ë‹¤ìŒ íƒêµ¬ë³´ê³ ì„œ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.

    **ì¤‘ìš” ì§€ì¹¨:**
    1. **ì›ë¬¸ ë‚´ìš© ë³´ì¡´**: í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì„ì˜ë¡œ ìš”ì•½í•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ê³ , ì›ë¬¸ì„ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ë³´ì¡´í•´ ì£¼ì„¸ìš”.
    2. **ìµœì†Œí•œì˜ ì •ë¦¬**: ì•„ë˜ ì‚¬í•­ë§Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”:
    - ë¬¸ì¥ ì¤‘ê°„ì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±° (ë‹¨ì–´ê°€ ëŠì–´ì§€ì§€ ì•Šë„ë¡)
    - ê·¸ë¦¼, í‘œ, ì‚¬ì§„ ìº¡ì…˜ ì œê±° (ì˜ˆ: "ê·¸ë¦¼ 1. xxx", "í‘œ 2. xxx" ë“±)
    - ê¹¨ì§„ í…ìŠ¤íŠ¸ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë¬¸ì ì œê±°
    3. **ì„¹ì…˜ ë¶„ë¥˜**: ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì„¹ì…˜ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ ì£¼ì„¸ìš”:
    - ì„œë¡  (ì—°êµ¬ ë™ê¸°, ì—°êµ¬ ëª©ì  í¬í•¨)
    - ì´ë¡ ì  ë°°ê²½ (ê´€ë ¨ ì´ë¡ , ì„ í–‰ì—°êµ¬ ì¡°ì‚¬ í¬í•¨)
    - ì—°êµ¬ ë°©ë²• (ì‹¤í—˜ ë°©ë²•, ì¤€ë¹„ë¬¼, ì‹¤í—˜ ê³¼ì • í¬í•¨)
    - ì—°êµ¬ ê²°ê³¼ (ì‹¤í—˜ ê²°ê³¼, ê´€ì°° ë‚´ìš© í¬í•¨)
    - ê²°ë¡  ë° ê³ ì°° (ê²°ë¡ , í† ì˜, í–¥í›„ ê³„íš í¬í•¨)
    - ë¶€ë¡ (ì°¸ê³ ë¬¸í—Œ, ì¶”ê°€ ìë£Œ í¬í•¨)

    **ë°˜í™˜ í˜•ì‹:**
    ```json
    [
        {
            "id": "ë³´ê³ ì„œë²ˆí˜¸_ì„¹ì…˜ëª…_ì²­í¬ë²ˆí˜¸",
            "text": "ì›ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ë³´ì¡´í•œ í…ìŠ¤íŠ¸...",
            "metadata": {
                "report_number": ë³´ê³ ì„œë²ˆí˜¸,
                "title": "ë³´ê³ ì„œ ì œëª©",
                "section": "ì„¹ì…˜ëª…",
                "chunk_index": ì²­í¬ë²ˆí˜¸
            }
        }
    ]
    ```

    **ì£¼ì˜ì‚¬í•­:**
    - ë°˜ë“œì‹œ ì™„ì „í•˜ê³  ìœ íš¨í•œ JSON ë°°ì—´ì„ ë°˜í™˜í•´ ì£¼ì„¸ìš”
    - ì›ë¬¸ì˜ ì˜ë¯¸ë‚˜ ë‚´ìš©ì„ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”
    - í…ìŠ¤íŠ¸ê°€ ê¸¸ë©´ ì ì ˆíˆ ë‚˜ëˆ„ë˜, ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ìœ ì§€í•´ ì£¼ì„¸ìš”
    """
except Exception as e:
    print(f"ì˜¤ë¥˜: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    exit()

# ì‚¬ìš©í•  Gemini ëª¨ë¸ ì„ íƒ ë° generation_config ì„¤ì •
generation_config = {
    "max_output_tokens": int(os.getenv("GEMINI_MAX_TOKENS", "32768")),
    "temperature": float(os.getenv("GEMINI_TEMPERATURE", "0.1")),
    "top_p": float(os.getenv("GEMINI_TOP_P", "0.9")),
    "top_k": int(os.getenv("GEMINI_TOP_K", "40"))
}

# ëª¨ë¸ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
gemini_model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite-preview-06-17")
model = genai.GenerativeModel(gemini_model, generation_config=generation_config)

# --- 1. íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • ---
INPUT_FOLDER = '../datas/extracted_pdf/union' 
OUTPUT_FOLDER = '../datas/json_results'
FILE_EXTENSION = '.txt'

# --- 2. í—¬í¼ í•¨ìˆ˜ ---

def extract_file_number(filename: str) -> str:
    """íŒŒì¼ëª…ì—ì„œ ë³´ê³ ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.match(r'(\d+)_.*\.txt', filename)
    return match.group(1) if match else "UNKNOWN"

def clean_json_string(text: str) -> str:
    """ëª¨ë¸ ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ ë°±í‹±ì„ ì œê±°í•©ë‹ˆë‹¤."""
    cleaned_text = text.strip()
    if cleaned_text.startswith("```json") and cleaned_text.endswith("```"):
        cleaned_text = cleaned_text[len("```json"):len(cleaned_text)-len("```")].strip()
    elif cleaned_text.startswith("```") and cleaned_text.endswith("```"):
        cleaned_text = cleaned_text[len("```"):len(cleaned_text)-len("```")].strip()
    return cleaned_text

def is_valid_json(json_str: str) -> bool:
    """JSON ë¬¸ìì—´ì´ ìœ íš¨í•œì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        json.loads(json_str)
        return True
    except json.JSONDecodeError:
        return False

def fix_incomplete_json(json_str: str) -> str:
    """ë¶ˆì™„ì „í•œ JSONì„ ìˆ˜ì •í•˜ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤."""
    json_str = json_str.strip()
    
    # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
    if json_str.endswith(','):
        json_str = json_str[:-1]
    
    # ë¯¸ì™„ì„± ê°ì²´ë‚˜ ë°°ì—´ ë‹«ê¸° ì‹œë„
    open_braces = json_str.count('{') - json_str.count('}')
    open_brackets = json_str.count('[') - json_str.count(']')
    
    # ì—´ë¦° ë¸Œë ˆì´ìŠ¤ ë‹«ê¸°
    for _ in range(open_braces):
        json_str += '}'
    
    # ì—´ë¦° ë¸Œë˜í‚· ë‹«ê¸°
    for _ in range(open_brackets):
        json_str += ']'
    
    return json_str

def chunk_text(text: str, max_chars: int = 15000) -> list:
    """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    if len(text) <= max_chars:
        return [text]
    
    chunks = []
    current_chunk = ""
    lines = text.split('\n')
    
    for line in lines:
        if len(current_chunk) + len(line) + 1 <= max_chars:
            current_chunk += line + '\n'
        else:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = line + '\n'
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def generate_timestamp() -> str:
    """í˜„ì¬ ì‹œê°„ì˜ timestampë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")

def get_report_metadata(report_number):
    db_path = "../datas/science_reports.db"
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute(
        "SELECT year, field, contest, award, authors, teacher, title FROM joined WHERE nttSn = ?",
        (str(report_number),)
    )
    row = cur.fetchone()
    conn.close()
    if row:
        return {
            "year": row[0],
            "field": row[1],
            "contest": row[2],
            "award": row[3],
            "authors": row[4],
            "teacher": row[5],
            "title": row[6]
        }
    else:
        return {}

def update_db_boolean_field(report_number: str, field_name: str, value: bool):
    """DBì— BOOLEAN í•„ë“œ ì—…ë°ì´íŠ¸"""
    try:
        db_path = "../datas/science_reports.db"
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute(f"""
            UPDATE joined SET {field_name} = ? WHERE nttSn = ?
        """, (1 if value else 0, report_number))
        conn.commit()
        conn.close()
        logger.info(f"âœ… DB ì—…ë°ì´íŠ¸ ì™„ë£Œ: nttSn={report_number}, {field_name}={value}")
    except Exception as e:
        logger.error(f"âŒ DB BOOLEAN í•„ë“œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (nttSn={report_number}, {field_name}): {e}")

# --- 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---

def analyze_file_with_stream(filepath: str, model, max_retries: int = 3):
    """
    íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    filename = os.path.basename(filepath)
    report_number = extract_file_number(filename)
    logger.info(f"ğŸ”„ ë¶„ì„ ì‹œì‘...")
    
    uploaded_file = None
    parsing_failures = []  # íŒŒì‹± ì‹¤íŒ¨ ëª©ë¡ ìˆ˜ì§‘

    try:
        # 1. íŒŒì¼ ë‚´ìš© ì½ê¸°
        with open(filepath, 'r', encoding='utf-8') as f:
            file_content = f.read()
        
        logger.info(f"- íŒŒì¼ í¬ê¸°: {len(file_content)} ë¬¸ì")
        
        # 2. í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í‚¹ ì²˜ë¦¬
        if len(file_content) > 20000:
            logger.info(f"- í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ ì²­í‚¹ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            chunks = chunk_text(file_content, max_chars=15000)
            logger.info(f"- {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            all_results = []
            for i, chunk in enumerate(chunks):
                logger.info(f"- ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
                chunk_result = process_chunk_with_stream(chunk, model, filename, report_number, i+1, parsing_failures)
                if chunk_result and isinstance(chunk_result, list):
                    all_results.extend(chunk_result)
                elif chunk_result and isinstance(chunk_result, dict):
                    all_results.append(chunk_result)
                
                # ì²­í¬ ê°„ ì§€ì—°
                time.sleep(2)
            
            # íŒŒì‹± ì‹¤íŒ¨ ëª©ë¡ì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ì¶œë ¥
            if parsing_failures:
                logger.warning(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨í•œ ì²­í¬: {', '.join(parsing_failures)}")
            
            return all_results
        else:
            # 3. íŒŒì¼ ì—…ë¡œë“œ
            logger.info(f"- íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
            uploaded_file = genai.upload_file(path=filepath, display_name=filename)
            logger.info(f"- ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.uri}")

            # 4. ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ì‹œë„
            for attempt in range(max_retries):
                logger.info(f"- ì‹œë„ {attempt + 1}/{max_retries}")
                
                try:
                    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    prompt_parts = [uploaded_file, prompt_template]

                    # API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¼ ë°©ì‹)
                    logger.info("Gemini APIë¡œ ìŠ¤íŠ¸ë¦¼ ë¶„ì„ ìš”ì²­ ì¤‘...")
                    response_stream = model.generate_content(prompt_parts, stream=True)
                    
                    # ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ìˆ˜ì§‘
                    full_response = ""
                    logger.info("ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ìˆ˜ì‹  ì¤‘...")
                    
                    chunk_count = 0
                    for chunk in response_stream:
                        if chunk.text:
                            full_response += chunk.text
                            chunk_count += 1
                            if chunk_count % 10 == 0:  # 10ê°œ ì²­í¬ë§ˆë‹¤ ì§„í–‰ ìƒí™© í‘œì‹œ
                                logger.info("ìŠ¤íŠ¸ë¦¼ ì§„í–‰ ì¤‘...")
                    
                    logger.info(f"ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ ì™„ë£Œ (ì´ ê¸¸ì´: {len(full_response)} ë¬¸ì)")
                    
                    if not full_response.strip():
                        logger.warning(f"ê²½ê³ : ë¹ˆ ì‘ë‹µ (ì‹œë„ {attempt + 1})")
                        continue

                    # JSON íŒŒì‹± ì‹œë„
                    clean_response_text = clean_json_string(full_response)
                    
                    # JSON ìœ íš¨ì„± ê²€ì‚¬
                    if not is_valid_json(clean_response_text):
                        logger.warning(f"ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šì€ JSON, ìˆ˜ì • ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1})")
                        fixed_json = fix_incomplete_json(clean_response_text)
                        if is_valid_json(fixed_json):
                            clean_response_text = fixed_json
                            logger.info("JSON ìˆ˜ì • ì„±ê³µ!")
                        else:
                            logger.warning(f"JSON ìˆ˜ì • ì‹¤íŒ¨, ë‹¤ì‹œ ì‹œë„... (ì‹œë„ {attempt + 1})")
                            continue

                    json_result = json.loads(clean_response_text)
                    logger.info("âœ… JSON ê²°ê³¼ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±.")

                    # ë©”íƒ€ë°ì´í„° ë³´ì • (nttSn ì„¤ì •)
                    meta_extra = get_report_metadata(report_number)
                    if isinstance(json_result, list):
                        for item in json_result:
                            if isinstance(item, dict):
                                if "metadata" not in item:
                                    item["metadata"] = {}
                                try:
                                    item["metadata"]["nttSn"] = int(report_number)
                                except ValueError:
                                    item["metadata"]["nttSn"] = report_number
                                item["metadata"].update(meta_extra)
                    elif isinstance(json_result, dict):
                        if "metadata" not in json_result:
                            json_result["metadata"] = {}
                        try:
                            json_result["metadata"]["nttSn"] = int(report_number)
                        except ValueError:
                            json_result["metadata"]["nttSn"] = report_number
                        json_result["metadata"].update(meta_extra)

                    return json_result

                except json.JSONDecodeError as e:
                    logger.warning(f"ê²½ê³ : JSON íŒŒì‹± ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        logger.info("3ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(3)
                    continue
                except Exception as e:
                    logger.error(f"ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}): {e}")
                    if attempt < max_retries - 1:
                        logger.info("3ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(3)
                    continue

            # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
            logger.error(f"âŒ {max_retries}ë²ˆì˜ ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")
            return {
                "error_type": "MAX_RETRIES_EXCEEDED",
                "filename": filename,
                "report_number": report_number,
                "attempts": max_retries
            }

    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ ({filename}): {e}")
        return {
            "error_type": "UNEXPECTED_ERROR",
            "filename": filename,
            "report_number": report_number,
            "exception_message": str(e)
        }
    finally:
        # ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
        if uploaded_file:
            logger.info(f"- ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ ì¤‘...")
            try:
                genai.delete_file(uploaded_file.name)
                logger.info(f"- íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
            except Exception as e:
                logger.warning(f"ê²½ê³ : ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def process_chunk_with_stream(chunk_text: str, model, filename: str, report_number: str, chunk_index: int, parsing_failures=None):
    """
    ë‹¨ì¼ ì²­í¬ë¥¼ ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if parsing_failures is None:
        parsing_failures = []
    try:
        # ì²­í¬ì— ëŒ€í•œ ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
        chunk_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” íƒêµ¬ë³´ê³ ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ ì£¼ì„¸ìš”.

**ì¤‘ìš”**: ì›ë¬¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ê³ , ê·¸ë¦¼/í‘œ ìº¡ì…˜ë§Œ ì œê±°í•´ ì£¼ì„¸ìš”.

ì„¹ì…˜: [ì„œë¡ , ì´ë¡ ì  ë°°ê²½, ì—°êµ¬ ë°©ë²•, ì—°êµ¬ ê²°ê³¼, ê²°ë¡  ë° ê³ ì°°, ë¶€ë¡] ì¤‘ ì„ íƒ

ë°˜í™˜ í˜•ì‹:
[
    {{
        "id": "{report_number}_ì„¹ì…˜ëª…_{chunk_index}",
        "text": "ì›ë¬¸ì„ ìµœëŒ€í•œ ë³´ì¡´í•œ í…ìŠ¤íŠ¸...",
        "metadata": {{
            "report_number": {report_number},
            "title": "ë³´ê³ ì„œ ì œëª©",
            "section": "ì„¹ì…˜ëª…",
            "chunk_index": {chunk_index}
        }}
    }}
]

í…ìŠ¤íŠ¸:
{chunk_text}
"""
        
        # ìŠ¤íŠ¸ë¦¼ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°
        response_stream = model.generate_content(chunk_prompt, stream=True)
        
        full_response = ""
        for chunk in response_stream:
            if chunk.text:
                full_response += chunk.text
        
        if not full_response.strip():
            return None
            
        clean_response_text = clean_json_string(full_response)
        
        if is_valid_json(clean_response_text):
            json_result = json.loads(clean_response_text)
        else:
            fixed_json = fix_incomplete_json(clean_response_text)
            if is_valid_json(fixed_json):
                json_result = json.loads(fixed_json)
            else:
                parsing_failures.append(f"ì²­í¬ {chunk_index}")
                return None
        
        # ì²­í¬ ê²°ê³¼ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ë³´ì • (nttSn ì„¤ì •)
        meta_extra = get_report_metadata(report_number)
        if isinstance(json_result, list):
            for item in json_result:
                if isinstance(item, dict):
                    if "metadata" not in item:
                        item["metadata"] = {}
                    try:
                        item["metadata"]["nttSn"] = int(report_number)
                    except ValueError:
                        item["metadata"]["nttSn"] = report_number
                    item["metadata"].update(meta_extra)
        elif isinstance(json_result, dict):
            if "metadata" not in json_result:
                json_result["metadata"] = {}
            try:
                json_result["metadata"]["nttSn"] = int(report_number)
            except ValueError:
                json_result["metadata"]["nttSn"] = report_number
            json_result["metadata"].update(meta_extra)
        
        return json_result
                
    except Exception as e:
        print(f"    ì˜¤ë¥˜: ì²­í¬ {chunk_index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---

def get_file_range_from_env():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ íŒŒì¼ ë²”ìœ„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    start_number = os.getenv('START_NTTSN')
    end_number = os.getenv('END_NTTSN')
    
    if start_number and end_number:
        try:
            return int(start_number), int(end_number)
        except ValueError:
            print(f"ì˜¤ë¥˜: ì˜ëª»ëœ nttSn ë²”ìœ„ í˜•ì‹: {start_number} ~ {end_number}")
            return None, None
    
    return None, None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²°ê³¼ ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)
        logger.info(f"ğŸ“ ì¶œë ¥ í´ë” ìƒì„±: {OUTPUT_FOLDER}")

    # ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    file_list = []
    if os.path.exists(INPUT_FOLDER):
        try:
            file_list = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(FILE_EXTENSION)]
            # íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            file_list = sorted(file_list)
        except (FileNotFoundError, PermissionError) as e:
            logger.warning(f"âš ï¸  ì…ë ¥ í´ë” '{INPUT_FOLDER}' ì ‘ê·¼ ì˜¤ë¥˜: {e}")
            file_list = []
    
    if not file_list:
        logger.warning(f"âš ï¸  '{INPUT_FOLDER}' í´ë”ì— ì²˜ë¦¬í•  '{FILE_EXTENSION}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("ë¹ˆ íŒŒì¼ ëª©ë¡ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        # ë¹ˆ íŒŒì¼ ëª©ë¡ìœ¼ë¡œ ê³„ì† ì§„í–‰

    # íŒŒì¼ ë²”ìœ„ ê²°ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    start_number, end_number = get_file_range_from_env()
    
    if start_number is None or end_number is None:
        logger.error("ì˜¤ë¥˜: í™˜ê²½ë³€ìˆ˜ë¡œ íŒŒì¼ ë²”ìœ„ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        logger.error("í™˜ê²½ë³€ìˆ˜: START_NTTSN, END_NTTSN")
        logger.error("ë³‘ë ¬ ì‹¤í–‰ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
        return
    
    logger.info(f"í™˜ê²½ë³€ìˆ˜ë¡œ ë°›ì€ ë²”ìœ„: {start_number} ~ {end_number}")
    
    # ë²”ìœ„ ê²€ì¦
    if start_number > end_number:
        logger.error("ì˜¤ë¥˜: ì‹œì‘ ë²ˆí˜¸ê°€ ë ë²ˆí˜¸ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # DBì—ì„œ ì‹¤ì œ íŒŒì¼ í™•ì¸ ë° í•„í„°ë§
    filtered_files = []
    try:
        db_path = "../datas/science_reports.db"
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        
        # ë²”ìœ„ ë‚´ ì‹¤ì œ íŒŒì¼ ì¡°íšŒ (json_api = 0ì¸ ê²ƒë§Œ)
        cur.execute("""
            SELECT nttSn FROM joined 
            WHERE nttSn BETWEEN ? AND ? AND (json_api = 0 OR json_api IS NULL)
            ORDER BY nttSn ASC
        """, (start_number, end_number))
        
        db_nttSn_list = [row[0] for row in cur.fetchall()]
        conn.close()
        
        logger.info(f"DBì—ì„œ ì¡°íšŒëœ nttSn ëª©ë¡: {len(db_nttSn_list)}ê°œ (json_api = 0ì¸ ê²ƒë§Œ)")
        
        # íŒŒì¼ ì‹œìŠ¤í…œì˜ íŒŒì¼ê³¼ DBì˜ nttSnì„ ë§¤ì¹­
        for filename in file_list:
            report_number = extract_file_number(filename)
            if report_number != "UNKNOWN":
                try:
                    file_number = int(report_number)
                    if file_number in db_nttSn_list:
                        filtered_files.append(filename)
                except ValueError:
                    continue
                    
    except Exception as e:
        logger.error(f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        # DB ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
        for filename in file_list:
            report_number = extract_file_number(filename)
            if report_number != "UNKNOWN":
                try:
                    file_number = int(report_number)
                    if start_number <= file_number <= end_number:
                        filtered_files.append(filename)
                except ValueError:
                    continue
    
    if not filtered_files:
        logger.error(f"ì˜¤ë¥˜: ë²ˆí˜¸ {start_number}~{end_number} ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"\n=== ì²˜ë¦¬ ì„¤ì • ===")
    logger.info(f"ì²˜ë¦¬ ë²”ìœ„: {start_number} ~ {end_number}")
    logger.info(f"ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(filtered_files)}ê°œ")
    logger.info(f"ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡:")
    for filename in filtered_files:
        logger.info(f"  - {filename}")
    
    # í™˜ê²½ë³€ìˆ˜ë¡œ ì‹¤í–‰ëœ ê²½ìš° ìë™ìœ¼ë¡œ ì§„í–‰
    logger.info(f"\ní™˜ê²½ë³€ìˆ˜ë¡œ ì‹¤í–‰ë˜ì–´ ìë™ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    logger.info(f"\nì´ {len(filtered_files)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜ ì¹´ìš´íŠ¸
    success_count = 0
    failed_count = 0
    failed_files = []  # ì‹¤íŒ¨í•œ íŒŒì¼ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸

    for index, filename in enumerate(filtered_files, 1):
        input_filepath = os.path.join(INPUT_FOLDER, filename)
        
        # íŒŒì¼ëª…ì—ì„œ ë³´ê³ ì„œ ë²ˆí˜¸ ì¶”ì¶œ
        report_number = extract_file_number(filename)
        
        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼ ì •ë³´ ì¶œë ¥
        logger.info(f"\nğŸ“„ [{index}/{len(filtered_files)}] íŒŒì¼ ì²˜ë¦¬ ì¤‘: {filename}")
        logger.info(f"   ë³´ê³ ì„œ ë²ˆí˜¸: {report_number}")
        logger.info("-" * 60)
        
        # API ë¶„ì„ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¼ ë°©ì‹)
        result_json = analyze_file_with_stream(input_filepath, model)

        # ê²°ê³¼ ì²˜ë¦¬ ë° ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
        if result_json and not isinstance(result_json, dict) or (isinstance(result_json, dict) and 'error_type' not in result_json):
            # ê°œë³„ JSON íŒŒì¼ëª… ìƒì„±
            output_filename = f"{report_number}_union.json"
            output_filepath = os.path.join(OUTPUT_FOLDER, output_filename)
            
            try:
                # ê²°ê³¼ë¥¼ ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥
                with open(output_filepath, 'w', encoding='utf-8') as f:
                    json.dump(result_json, f, ensure_ascii=False, indent=4)
                logger.info(f"  âœ… '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ")
                
                # ì„±ê³µ ì‹œ DB ì—…ë°ì´íŠ¸
                update_db_boolean_field(report_number, 'json_api', True)
                
                success_count += 1
            except Exception as e:
                logger.error(f"  âŒ '{output_filename}' ì €ì¥ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ DB ì—…ë°ì´íŠ¸
                update_db_boolean_field(report_number, 'json_api', False)
                failed_count += 1
                failed_files.append(report_number)
        else:
            # ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
            if isinstance(result_json, dict) and 'error_type' in result_json:
                logger.error(f"âŒ API ì²˜ë¦¬ ì‹¤íŒ¨: {result_json.get('error_type', 'UNKNOWN_ERROR')} - {filename}")
                logger.error(f"   ì—ëŸ¬ ìƒì„¸: {result_json}")
            else:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨ ë˜ëŠ” ê±´ë„ˆëœ€: {filename}")
            
            # ì‹¤íŒ¨ ì‹œ DB ì—…ë°ì´íŠ¸
            update_db_boolean_field(report_number, 'json_api', False)
            failed_count += 1
            failed_files.append(report_number)
        
        # API Rate Limitë¥¼ í”¼í•˜ê¸° ìœ„í•œ ì§€ì—°
        time.sleep(5)

    # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    logger.info(f"\nâœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"   - ì„±ê³µ: {success_count}ê°œ íŒŒì¼")
    logger.info(f"   - ì‹¤íŒ¨: {failed_count}ê°œ íŒŒì¼")
    logger.info(f"   - ì´ ì²˜ë¦¬: {len(filtered_files)}ê°œ íŒŒì¼")
    logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: '{OUTPUT_FOLDER}' í´ë”")
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
    if failed_files:
        logger.error(f"âŒ ì‹¤íŒ¨í•œ íŒŒì¼ ë²ˆí˜¸: {failed_files}")
        logger.error(f"   ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜: {len(failed_files)}ê°œ")
    else:
        logger.info(f"âœ… ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main() 