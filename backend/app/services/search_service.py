import os
import sys
import warnings
import torch
from google import genai
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
import numpy as np
import re
from typing import List, Dict, Any, Optional, Tuple
from fastapi import HTTPException
from langchain_core.prompts import PromptTemplate
from .logger_service import LoggerService

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings('ignore', category=RuntimeWarning, module='sklearn')

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ê°€ì¤‘ì¹˜ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜¤ê¸°)
def get_weight_config():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì¤‘ì¹˜ ì„¤ì •ì„ ì½ì–´ì™€ì„œ ë°˜í™˜"""
    return {
        "alpha": float(os.getenv("SEARCH_ALPHA", "1.1")),                    # ì œëª© ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (content_score + alpha * title_score)
        "gamma": float(os.getenv("SEARCH_GAMMA", "0.05")),                   # í‚¤ì›Œë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜ (gamma * matched_terms)
        "section_weights": {  # ì„¹ì…˜ ìš°ì„ ìˆœìœ„ë³„ ê°€ì¤‘ì¹˜
            0: float(os.getenv("SEARCH_SECTION_WEIGHT_1", "0.08")),  # 1ìˆœìœ„
            1: float(os.getenv("SEARCH_SECTION_WEIGHT_2", "0.04")),  # 2ìˆœìœ„
            2: float(os.getenv("SEARCH_SECTION_WEIGHT_3", "0.01"))   # 3ìˆœìœ„
        },
    "award_weights": {               # ìˆ˜ìƒë„ë³„ ê°€ì¤‘ì¹˜ (ë†’ì€ ìˆœìœ„ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
            "ëŒ€í†µë ¹ìƒ": float(os.getenv("SEARCH_AWARD_PRESIDENT", "0.15")),
            "êµ­ë¬´ì´ë¦¬ìƒ": float(os.getenv("SEARCH_AWARD_PRIME_MINISTER", "0.12")),
            "ìµœìš°ìˆ˜ìƒ": float(os.getenv("SEARCH_AWARD_EXCELLENT", "0.10")),
            "íŠ¹ìƒ": float(os.getenv("SEARCH_AWARD_SPECIAL", "0.08")),
            "ìš°ìˆ˜ìƒ": float(os.getenv("SEARCH_AWARD_GOOD", "0.06")),
            "ì¥ë ¤ìƒ": float(os.getenv("SEARCH_AWARD_ENCOURAGEMENT", "0.04"))
    },
    "metadata_weights": {
            "field": float(os.getenv("SEARCH_METADATA_FIELD", "0.06")),      # ë¶„ì•¼ ë§¤ì¹­ ê°€ì¤‘ì¹˜
            "year": float(os.getenv("SEARCH_METADATA_YEAR", "0.05")),        # ì—°ë„ ë§¤ì¹­ ê°€ì¤‘ì¹˜
            "award": float(os.getenv("SEARCH_METADATA_AWARD", "0.08")),      # ìˆ˜ìƒë‚´ì—­ ë§¤ì¹­ ê°€ì¤‘ì¹˜
            "authors": float(os.getenv("SEARCH_METADATA_AUTHORS", "0.1")),   # ì €ì ë§¤ì¹­ ê°€ì¤‘ì¹˜
            "teacher": float(os.getenv("SEARCH_METADATA_TEACHER", "0.1"))    # ì§€ë„êµì‚¬ ë§¤ì¹­ ê°€ì¤‘ì¹˜
    }
}

# ì „ì—­ ê°€ì¤‘ì¹˜ ì„¤ì • ë³€ìˆ˜
WEIGHT_CONFIG = get_weight_config()

# Gemini API ì„¤ì •
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
client = genai.Client(api_key=API_KEY)

# ì„ë² ë”© ëª¨ë¸ ë° Chroma DB ì„¤ì •
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
EMBEDDING_DEVICE = os.getenv("EMBEDDING_DEVICE", "cuda" if torch.cuda.is_available() else "cpu")

EMBEDDING_BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "32"))

# ê²½ë¡œ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜¤ê¸°)
def get_paths():
    """í™˜ê²½ë³€ìˆ˜ì—ì„œ ê²½ë¡œ ì„¤ì •ì„ ì½ì–´ì™€ì„œ ë°˜í™˜"""
    current_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(os.path.dirname(current_dir))
    return {
        "chroma_db": os.path.join(base_dir, os.getenv("CHROMA_DB_PATH", "datas/chroma_db")),
        "extracted_pdf": os.path.join(base_dir, os.getenv("EXTRACTED_PDF_PATH", "datas/extracted_pdf")),
        "pdf_reports": os.path.join(base_dir, os.getenv("PDF_REPORTS_PATH", "datas/pdf_reports")),
        "json_results": os.path.join(base_dir, os.getenv("JSON_RESULTS_PATH", "datas/json_results")),
        "science_reports_db": os.path.join(base_dir, os.getenv("SCIENCE_REPORTS_DB_PATH", "datas/science_reports.db")),
        "prompts": os.path.join(base_dir, os.getenv("PROMPTS_PATH", "prompts"))
    }

# ì „ì—­ ê²½ë¡œ ì„¤ì •
PATHS = get_paths()
CHROMA_DIR = PATHS["chroma_db"]

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
try:
    prompt_path = os.path.join(PATHS["prompts"], "prompt_search.txt")
    with open(prompt_path, "r", encoding="utf-8") as f:
        QUERY_ANALYSIS_PROMPT_TEMPLATE = f.read()
except FileNotFoundError:
    raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")

QUERY_ANALYSIS_PROMPT = PromptTemplate.from_template(QUERY_ANALYSIS_PROMPT_TEMPLATE)

# ì „ì—­ ë³€ìˆ˜
_embedding_model = None
_vectorstore = None


def cosine_similarity_numpy(vec1, vec2):
    """numpyë¥¼ ì‚¬ìš©í•œ cosine similarity ê³„ì‚°"""
    if np.allclose(vec1, 0) or np.allclose(vec2, 0):
        return 0.0
    epsilon = 1e-8
    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + epsilon)
    return np.clip(similarity, -1.0, 1.0)


def get_image_path_from_db(number: str) -> str:
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë³´ê³ ì„œ ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ í™•ì¸
        image_dir = os.path.join(PATHS["extracted_pdf"], "image")
        image_path = os.path.join(image_dir, f"{number}_image.png")
        
        if os.path.exists(image_path):
            return f"{number}_image.png"
        else:
            return ""
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨ (ë²ˆí˜¸: {number}): {e}")
        return ""


class SearchService:
    """ê²€ìƒ‰ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def initialize_models():
        """ëª¨ë¸ë“¤ì„ ì§€ì—° ì´ˆê¸°í™”"""
        global _embedding_model, _vectorstore
        
        if _embedding_model is None:
            print(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if not torch.cuda.is_available():
                print(f"âŒ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                print(f"GPU ë“œë¼ì´ë²„ì™€ PyTorch CUDA ë²„ì „ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                raise RuntimeError("CUDAê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
            
            # Gemini API ì„¤ì •
            # genai.configure(api_key=API_KEY) # ì´ì „ ì½”ë“œì—ì„œ ì´ë¯¸ ì„¤ì •ë¨
            print(f"âœ… Gemini API ì„¤ì • ì™„ë£Œ")
            
            # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (CUDA í•„ìˆ˜)
            print(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {EMBEDDING_MODEL_NAME}")
            try:
                _embedding_model = HuggingFaceEmbeddings(
                    model_name=EMBEDDING_MODEL_NAME,
                    model_kwargs={'device': EMBEDDING_DEVICE},
                    encode_kwargs={'batch_size': EMBEDDING_BATCH_SIZE}
                )
                print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {EMBEDDING_DEVICE}")
            except RuntimeError as e:
                print(f"âŒ CUDA ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"CUDAê°€ í•„ìš”í•©ë‹ˆë‹¤. GPU ë“œë¼ì´ë²„ì™€ PyTorch CUDA ë²„ì „ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                raise RuntimeError(f"CUDA ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜: {e}")
            
            # Chroma DB ì´ˆê¸°í™”
            print(f"ğŸ’¾ Chroma DB ë¡œë”© ì¤‘: {CHROMA_DIR}")
            if not os.path.exists(CHROMA_DIR):
                raise ValueError(f"Chroma DB ë””ë ‰í† ë¦¬ '{CHROMA_DIR}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            _vectorstore = Chroma(
                persist_directory=CHROMA_DIR, 
                embedding_function=_embedding_model,
                collection_name="my_report_collection"
            )
            print(f"âœ… Chroma DB ë¡œë”© ì™„ë£Œ")
        
        return _embedding_model, _vectorstore
    
    @staticmethod
    async def analyze_user_query(original_query: str, user_id: str, logger_service: Optional[LoggerService] = None, is_hidden: bool = False) -> Tuple[str, List[str], List[str], Dict, str, Dict]:
        """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ìš”ì•½ ì¿¼ë¦¬, ìš°ì„ ìˆœìœ„ ì„¹ì…˜, í‚¤ì›Œë“œ, ë©”íƒ€ë°ì´í„° í•„í„°, ì˜ë„, ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„°ë¥¼ ë°˜í™˜"""
        generation_config = {
            "max_output_tokens": int(os.getenv("GEMINI_MAX_TOKENS", "2048")),
            "temperature": float(os.getenv("GEMINI_TEMPERATURE", "0.1")),
            "top_p": float(os.getenv("GEMINI_TOP_P", "0.9")),
            "top_k": int(os.getenv("GEMINI_TOP_K", "40"))
        }
        gemini_model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-flash-latest")
        prompt = QUERY_ANALYSIS_PROMPT_TEMPLATE.replace("{{query}}", original_query)

        try:
            response = client.models.generate_content(
                model=gemini_model_name,
                contents=prompt,
            )
            response_content = response.text
            
            # ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            usage_metadata = {}
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage_metadata = {
                    'total_token_count': response.usage_metadata.total_token_count,
                    'prompt_token_count': response.usage_metadata.prompt_token_count,
                    'candidates_token_count': response.usage_metadata.candidates_token_count
                }
            
            print(f"ğŸ” Gemini API ì‘ë‹µ:")
            print(response_content)
            print(f"ğŸ” ì‚¬ìš©ëŸ‰ ë©”íƒ€ë°ì´í„°: {usage_metadata}")

            summary_query = ""
            priority_sections = []
            extracted_keywords = []

            summary_match = re.search(r"ìš”ì•½ ì¿¼ë¦¬:\s*(.+)", response_content)
            if summary_match:
                summary_query = summary_match.group(1).strip()

            sections_match = re.search(r"ìš°ì„ ìˆœìœ„ ì„¹ì…˜:\s*(.+)", response_content)
            if sections_match:
                sections_str = sections_match.group(1).strip()
                priority_sections = [s.strip() for s in sections_str.split('>') if s.strip()]

            keywords_match = re.search(r"í•µì‹¬ í‚¤ì›Œë“œ:\s*(.+)", response_content)
            if keywords_match:
                keyword_str = keywords_match.group(1).strip()
                extracted_keywords = [kw.strip().lower() for kw in re.split(r"[,\s]+", keyword_str) if len(kw.strip()) > 1]

            metadata_filters = {}
            for key in ["field", "year", "award", "authors", "teacher"]:
                match = re.search(rf"{key}:\s*(.+)", response_content, re.IGNORECASE)
                if match:
                    metadata_filters[key] = match.group(1).strip()

            # ë¡œê¹… ìˆ˜í–‰ (logger_serviceê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ)
            if logger_service and usage_metadata:
                try:
                    await logger_service.log_ai_usage(
                        user_id=user_id,
                        service_name="query_summary",
                        request_prompt=original_query,
                        request_token_count=usage_metadata.get('prompt_token_count', 0),
                        response_token_count=usage_metadata.get('candidates_token_count', 0),
                        total_token_count=usage_metadata.get('total_token_count', 0),
                        is_hidden=is_hidden
                    )
                except Exception as log_error:
                    print(f"âŒ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {log_error}")
            
            return summary_query, priority_sections, extracted_keywords, metadata_filters, usage_metadata

        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return original_query, [], [], {}, "", {}
    
    @staticmethod
    def rerank_with_weights(
        query_embedding,
        documents,
        embedding_model,
        priority_sections,
        simplified_query,
        original_query,
        keyword_match_terms,
        weight_config=WEIGHT_CONFIG,
        metadata_filters=None
    ):
        """ë¬¸ì„œë¥¼ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ì¬ì •ë ¬"""
        alpha = weight_config["alpha"]
        gamma = weight_config["gamma"]
        section_weights = weight_config["section_weights"]
        award_weights = weight_config["award_weights"]  # ìˆ˜ìƒë„ë³„ ê°€ì¤‘ì¹˜
        metadata_weight_table = weight_config["metadata_weights"]

        doc_texts = [doc.page_content for doc in documents]
        doc_titles = [doc.metadata.get("title", "") for doc in documents]

        doc_vectors = embedding_model.embed_documents(doc_texts)
        title_vectors = embedding_model.embed_documents(doc_titles)

        simplified_terms = [t.lower() for t in simplified_query.split() if len(t) > 1]
        original_terms = [t.lower() for t in original_query.split() if len(t) > 1]

        reranked_results = []

        for i, doc in enumerate(documents):
            content_score = cosine_similarity_numpy(query_embedding, doc_vectors[i])
            title_score = cosine_similarity_numpy(query_embedding, title_vectors[i]) if doc_titles[i].strip() else 0.0

            # ì„¹ì…˜ ë¶€ìŠ¤í„° ì ìˆ˜ ê³„ì‚° (Geminiê°€ ë¶„ì„í•œ ìš°ì„ ìˆœìœ„ ì„¹ì…˜ê³¼ ì¼ì¹˜í•  ë•Œ ê°€ì¤‘ì¹˜ ì ìš©)
            section_boost_score = 0.0
            doc_section = doc.metadata.get("section", "")
            if doc_section in priority_sections:
                try:
                    priority_index = priority_sections.index(doc_section)  # ìš°ì„ ìˆœìœ„ì—ì„œì˜ ìœ„ì¹˜ (0, 1, 2)
                    section_boost_score = section_weights.get(priority_index, 0.0)  # 1ìˆœìœ„: 0.08, 2ìˆœìœ„: 0.04, 3ìˆœìœ„: 0.01
                except ValueError:
                    pass

            text_lower = doc.page_content.lower()
            title_lower = doc_titles[i].lower()

            simplified_matches = sum(1 for term in simplified_terms if term in text_lower or term in title_lower)
            original_matches = sum(1 for term in original_terms if term in text_lower or term in title_lower)
            matched_terms = simplified_matches + original_matches

            keyword_matches = sum(1 for kw in keyword_match_terms if kw in text_lower or kw in title_lower)
            keyword_boost_score = gamma * (matched_terms + keyword_matches)

            # ìˆ˜ìƒë„ ë¶€ìŠ¤í„° ì ìˆ˜ ê³„ì‚° (ë†’ì€ ìˆœìœ„ì˜ ìˆ˜ìƒì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš©)
            award_boost_score = 0.0
            doc_award = doc.metadata.get("award", "")
            if doc_award in award_weights:
                award_boost_score = award_weights[doc_award]  # ëŒ€í†µë ¹ìƒ: 0.15, êµ­ë¬´ì´ë¦¬ìƒ: 0.12, ìµœìš°ìˆ˜ìƒ: 0.10, íŠ¹ìƒ: 0.08, ìš°ìˆ˜ìƒ: 0.06, ì¥ë ¤ìƒ: 0.04

            metadata_boost = 0.0
            if metadata_filters:
                for key, val in metadata_filters.items():
                    doc_val = str(doc.metadata.get(key, '')).strip()
                    if doc_val == val:
                        metadata_boost += metadata_weight_table.get(key, 0.05)

            matched_terms += keyword_matches
            total_score = (
                (content_score + alpha * title_score)
                + section_boost_score
                + keyword_boost_score
                + award_boost_score  # ìˆ˜ìƒë„ ë¶€ìŠ¤í„° ì ìˆ˜ ì¶”ê°€
                + metadata_boost
            )

            score_info = {
                'content_score': content_score,        # ë‚´ìš© ìœ ì‚¬ë„ ì ìˆ˜ (0~1)
                'title_score': title_score,            # ì œëª© ìœ ì‚¬ë„ ì ìˆ˜ (0~1)
                'alpha': alpha,                        # ì œëª© ê°€ì¤‘ì¹˜ ê³„ìˆ˜
                'section_boost_score': section_boost_score,  # ì„¹ì…˜ ë¶€ìŠ¤í„° ì ìˆ˜ (1ìˆœìœ„: 0.08, 2ìˆœìœ„: 0.04, 3ìˆœìœ„: 0.01)
                'keyword_boost_score': keyword_boost_score,  # í‚¤ì›Œë“œ ë§¤ì¹­ ë¶€ìŠ¤í„° ì ìˆ˜
                'award_boost_score': award_boost_score,  # ìˆ˜ìƒë„ ë¶€ìŠ¤í„° ì ìˆ˜ (ëŒ€í†µë ¹ìƒ: 0.15, êµ­ë¬´ì´ë¦¬ìƒ: 0.12, ìµœìš°ìˆ˜ìƒ: 0.10, íŠ¹ìƒ: 0.08, ìš°ìˆ˜ìƒ: 0.06, ì¥ë ¤ìƒ: 0.04)
                'metadata_boost': metadata_boost,      # ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ë¶€ìŠ¤í„° ì ìˆ˜
                'matched_terms': matched_terms,        # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜
                'simplified_matches': simplified_matches,  # ìš”ì•½ ì¿¼ë¦¬ ë§¤ì¹­ ê°œìˆ˜
                'original_matches': original_matches,  # ì›ë³¸ ì¿¼ë¦¬ ë§¤ì¹­ ê°œìˆ˜
                'total_score': total_score,            # ìµœì¢… ì ìˆ˜
                'calculation': f"({content_score:.4f} + {alpha} Ã— {title_score:.4f}) + {section_boost_score:.4f} + {keyword_boost_score:.4f} + {award_boost_score:.4f} + {metadata_boost:.4f} = {total_score:.4f}"
            }

            reranked_results.append((doc, total_score, score_info))

        reranked_results.sort(key=lambda x: x[1], reverse=True)
        return reranked_results
    
    @staticmethod
    async def search_documents(query: str, k: int = 10, user_id: Optional[str] = None, logger_service: Optional[LoggerService] = None, is_hidden: bool = False) -> Dict[str, Any]:
        """ë¬¸ì„œ ê²€ìƒ‰ ì²˜ë¦¬"""
        try:
            print(f"ğŸ” ê²€ìƒ‰ ìš”ì²­ ë°›ìŒ: {query}")
            query = query.strip()

            if not query:
                raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

            print(f"ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            # ëª¨ë¸ ì´ˆê¸°í™”
            embedding_model, vectorstore = SearchService.initialize_models()
            print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

            print(f"ğŸ§  ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘...")
            # ì¿¼ë¦¬ ë¶„ì„
            summary_query, priority_sections, keyword_terms, metadata_filters, usage_metadata = await SearchService.analyze_user_query(query, user_id, logger_service, is_hidden)
            print(f"âœ… ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: {summary_query}")
            # print(f"ğŸ¯ ì¿¼ë¦¬ ì˜ë„: {intent if intent else 'ë¶„ì„ ë¶ˆê°€'}")
            
            if not summary_query:
                raise HTTPException(status_code=500, detail="ìš”ì•½ ì¿¼ë¦¬ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

            print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘...")
            # ë²¡í„° ê²€ìƒ‰
            initial_search_k = k * 5
            raw_results = vectorstore.similarity_search_with_score(summary_query, k=initial_search_k)
            raw_documents = [doc for doc, _ in raw_results]
            print(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(raw_documents)}ê°œ ë¬¸ì„œ ë°œê²¬")

            # ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
            if metadata_filters:
                filtered_documents = [doc for doc in raw_documents if all(str(doc.metadata.get(key, '')).strip() == val.strip() for key, val in metadata_filters.items())]
                if len(filtered_documents) < k:
                    filtered_documents = raw_documents
            else:
                filtered_documents = raw_documents

            query_embedding = embedding_model.embed_query(summary_query)

            # ì¬ì •ë ¬
            reranked = SearchService.rerank_with_weights(
                query_embedding,
                filtered_documents,
                embedding_model,
                priority_sections,
                summary_query,
                query,
                keyword_terms,
                WEIGHT_CONFIG,
                metadata_filters
            )

            # ê²°ê³¼ í¬ë§·íŒ… (ì¤‘ë³µ number ì œê±°)
            results = []
            seen_numbers = set()
            for i, (doc, score, score_info) in enumerate(reranked, 1):
                number = str(doc.metadata.get('nttSn', 'N/A'))  # í•­ìƒ ë¬¸ìì—´ë¡œ ë³€í™˜
                if number in seen_numbers:
                    continue
                seen_numbers.add(number)
                
                # ì´ë¯¸ì§€ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
                image_path = get_image_path_from_db(number)
                
                result = {
                    'rank': len(results) + 1,
                    'title': doc.metadata.get('title', 'N/A'),
                    'section': doc.metadata.get('section', 'N/A'),
                    'number': number,
                    'metadata': {
                        'field': doc.metadata.get('field', 'N/A'),
                        'year': doc.metadata.get('year', 'N/A'),
                        'award': doc.metadata.get('award', 'N/A'),
                        'authors': doc.metadata.get('authors', 'N/A'),
                        'teacher': doc.metadata.get('teacher', 'N/A'),
                        'source_type': doc.metadata.get('source_type', 'N/A')
                    },
                    'score_info': score_info,
                    'content': doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content,
                    'image_path': image_path
                }
                results.append(result)
                if len(results) >= k:
                    break

            response_data = {
                "query": query,
                "summary_query": summary_query,
                "priority_sections": priority_sections,
                "metadata_filters": metadata_filters,
                # "intent": intent,
                "total_results": len(results),
                "results": results,
                "usage_metadata": usage_metadata
            }
            
            print(f"ğŸ” API ì‘ë‹µ ë°ì´í„°:")
            # print(f"  - intent: '{intent}'")
            print(f"  - total_results: {len(results)}")
            print("-" * 50)
            
            return response_data

        except HTTPException:
            raise
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}") 